{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "=====================================================================\n",
        "MMCTR - Multimodal Click-Through Rate Prediction Pipeline\n",
        "=====================================================================\n",
        "\n",
        "\n",
        "* Author: Charkaoui Wissal\n",
        "* Date: December 2024\n",
        "* Competition: MicroLens 1M MMCTR Challenge\n",
        "\n",
        "**Description:**\n",
        "    Ce pipeline combine des embeddings texte (BERT) et image (CLIP) pour\n",
        "    prédire le taux de clic (CTR) en utilisant un modèle Deep Interest Network.\n",
        "    \n",
        "\n",
        "**Architecture:**\n",
        "    \n",
        "    1. Feature Engineering: Fusion BERT + CLIP → PCA 128D\n",
        "    2. DIN Model: Attention-based user interest modeling\n",
        "    3. Training: 10 epochs avec validation AUC\n",
        "    4. Submission: Format officiel (ID, Task2)\n",
        "\n",
        "**Notes importantes:**\n",
        "\n",
        "    - Tout est en float32 pour éviter les problèmes PyArrow\n",
        "    - Le padding_idx=0 gère les items manquants\n",
        "    - Utilise Dice activation (meilleur que ReLU pour ce cas)\n",
        "    - Les poids de fusion 0.55/0.45 ont été optimisés empiriquement\n"
      ],
      "metadata": {
        "id": "im-wBfWgJKKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# INSTALLATION DES PACKAGES\n",
        "# =========================\n",
        "# Note: Exécutez cette cellule UNE SEULE FOIS dans Colab\n",
        "!pip install -q polars scikit-learn numpy pyarrow tqdm open_clip_torch Pillow\n",
        "\n",
        "# =========================\n",
        "# IMPORTS\n",
        "# =========================\n",
        "import os, gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import polars as pl  # Plus rapide que pandas pour les gros fichiers\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from PIL import Image\n",
        "import open_clip  # CLIP pour les embeddings d'images\n",
        "from tqdm import tqdm  # Barres de progression\n",
        "\n",
        "# Pour Google Colab uniquement\n",
        "from google.colab import drive\n",
        "\n",
        "# =========================\n",
        "# MONTAGE GOOGLE DRIVE\n",
        "# =========================\n",
        "# Vos données doivent être dans MyDrive/competition/MicroLens_1M_MMCTR/\n",
        "drive.mount('/content/mydrive')\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION DES CHEMINS\n",
        "# =========================\n",
        "# Structure attendue du dataset:\n",
        "# MyDrive/competition/MicroLens_1M_MMCTR/\n",
        "#   ├── MicroLens_1M_x1/\n",
        "#   │   ├── train.parquet\n",
        "#   │   ├── valid.parquet\n",
        "#   │   ├── test.parquet\n",
        "#   │   └── item_info.parquet\n",
        "#   ├── item_feature.parquet (contient txt_emb_BERT)\n",
        "#   └── item_images/item_image/*.jpg\n",
        "\n",
        "BASE = \"/content/mydrive/MyDrive/competition/MicroLens_1M_MMCTR\"\n",
        "DATA_DIR = os.path.join(BASE, \"MicroLens_1M_x1\")\n",
        "IMG_DIR = os.path.join(BASE, \"item_images/item_image\")\n",
        "MODEL_DIR = os.path.join(BASE, \"models\")\n",
        "PRED_DIR = os.path.join(BASE, \"predictions\")\n",
        "\n",
        "# Créer les dossiers de sortie s'ils n'existent pas\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(PRED_DIR, exist_ok=True)\n",
        "\n",
        "# Détection automatique du device (GPU si disponible)\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if DEVICE == 'cuda':\n",
        "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# ======================================================================\n",
        "# PART 1 — FEATURE ENGINEERING (BERT + CLIP FUSION)\n",
        "# ======================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PART 1: Extraction et fusion des embeddings multimodaux\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ---------- Charger les embeddings BERT (texte)\n",
        "print(\"\\nChargement des embeddings BERT...\")\n",
        "feat_path = os.path.join(BASE, \"item_feature.parquet\")\n",
        "df_feat = pd.read_parquet(feat_path)\n",
        "item_ids = df_feat['item_id'].values\n",
        "\n",
        "# IMPORTANT: Conversion explicite en float32 pour économiser la mémoire\n",
        "# et éviter les problèmes de compatibilité PyArrow/PyTorch\n",
        "# Sans cette conversion, numpy utilise float64 par défaut, ce qui double\n",
        "# la consommation mémoire et peut causer des erreurs lors de la sauvegarde Parquet\n",
        "bert_emb = np.stack(df_feat['txt_emb_BERT'].values).astype(np.float32)\n",
        "\n",
        "print(f\"   BERT embeddings loaded: shape {bert_emb.shape}\")\n",
        "print(f\"   Number of items: {len(item_ids)}\")\n",
        "\n",
        "# ---------- Dataset PyTorch pour les images\n",
        "class ImageDS(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset PyTorch pour charger et transformer les images d'items.\n",
        "\n",
        "    Cette classe gère automatiquement plusieurs cas edge:\n",
        "        - Différents formats d'images (.jpg, .png, .jpeg)\n",
        "        - Images manquantes (crée une image grise 128,128,128 par défaut)\n",
        "        - Normalisation ImageNet standard requise pour CLIP\n",
        "\n",
        "    La normalisation utilise les statistiques ImageNet car CLIP a été\n",
        "    pré-entraîné sur ce dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, ids):\n",
        "        self.ids = ids\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "        # Transformations standards pour CLIP (ViT-B/32)\n",
        "        # Ces valeurs de normalisation sont les statistiques d'ImageNet\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Taille fixe requise par CLIP\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
        "                std=[0.229, 0.224, 0.225]    # ImageNet std\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        iid = self.ids[i]\n",
        "        path = None\n",
        "\n",
        "        # Chercher l'image avec différentes extensions possibles\n",
        "        for ext in ['.jpg', '.png', '.jpeg']:\n",
        "            p = os.path.join(self.img_dir, f\"{iid}{ext}\")\n",
        "            if os.path.exists(p):\n",
        "                path = p\n",
        "                break\n",
        "\n",
        "        # Si image trouvée, la charger en RGB\n",
        "        # Sinon, créer une image grise neutre pour éviter les erreurs\n",
        "        if path:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "        else:\n",
        "            # Image de remplacement: gris moyen (128,128,128)\n",
        "            # Cette approche est meilleure que de crasher ou de skip l'item\n",
        "            img = Image.new('RGB', (224, 224), (128, 128, 128))\n",
        "\n",
        "        return self.tf(img)\n",
        "\n",
        "# ---------- Extraire les embeddings CLIP (images)\n",
        "print(\"\\nExtraction des embeddings CLIP...\")\n",
        "img_ds = ImageDS(IMG_DIR, item_ids)\n",
        "img_dl = DataLoader(\n",
        "    img_ds,\n",
        "    batch_size=32,      # Batch size raisonnable pour l'extraction\n",
        "    shuffle=False,      # Pas besoin de shuffle pour l'extraction\n",
        "    num_workers=2       # Parallélisation du chargement\n",
        ")\n",
        "\n",
        "# Charger le modèle CLIP pré-entraîné\n",
        "# ViT-B/32 est un bon compromis entre performance et vitesse\n",
        "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
        "    'ViT-B-32',         # Architecture Vision Transformer\n",
        "    pretrained='openai' # Poids pré-entraînés officiels d'OpenAI\n",
        ")\n",
        "clip_model = clip_model.to(DEVICE).eval()\n",
        "\n",
        "@torch.no_grad()  # Désactive le calcul des gradients (pas nécessaire pour l'extraction)\n",
        "def extract_clip(dl):\n",
        "    \"\"\"\n",
        "    Extrait les embeddings CLIP pour toutes les images du dataset.\n",
        "\n",
        "    Cette fonction traite les images par batch pour l'efficacité mémoire.\n",
        "    Le décorateur @torch.no_grad() réduit la consommation mémoire de ~50%.\n",
        "\n",
        "    Args:\n",
        "        dl: DataLoader contenant les images transformées\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Embeddings de forme (n_items, 512) en float32\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for x in tqdm(dl, desc=\"Extracting CLIP embeddings\"):\n",
        "        x = x.to(DEVICE)\n",
        "        # encode_image retourne des embeddings 512D normalisés\n",
        "        emb = clip_model.encode_image(x).cpu().numpy().astype(np.float32)\n",
        "        out.append(emb)\n",
        "    return np.vstack(out)\n",
        "\n",
        "clip_emb = extract_clip(img_dl)\n",
        "print(f\"   CLIP embeddings extracted: shape {clip_emb.shape}\")\n",
        "\n",
        "# Libérer la mémoire GPU immédiatement après extraction\n",
        "# Important dans Colab où la RAM GPU est limitée\n",
        "del clip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"   GPU memory cleared\")\n",
        "\n",
        "# ---------- Fusion des embeddings BERT + CLIP\n",
        "print(\"\\nFusion des embeddings multimodaux...\")\n",
        "\n",
        "# Alignement des dimensions\n",
        "# CLIP produit 512D, BERT produit 768D\n",
        "# On pad CLIP avec des zéros pour matcher les dimensions\n",
        "if clip_emb.shape[1] != bert_emb.shape[1]:\n",
        "    print(f\"   Padding CLIP embeddings: {clip_emb.shape[1]}D -> {bert_emb.shape[1]}D\")\n",
        "    clip_emb = np.pad(\n",
        "        clip_emb,\n",
        "        ((0, 0), (0, bert_emb.shape[1] - clip_emb.shape[1])),\n",
        "        constant_values=0\n",
        "    )\n",
        "\n",
        "# Normalisation L2 avant fusion\n",
        "# IMPORTANT: Ceci est crucial pour que la fusion pondérée soit équitable\n",
        "# Sans normalisation, la modalité avec les plus grandes valeurs dominerait\n",
        "bert_emb /= np.linalg.norm(bert_emb, axis=1, keepdims=True) + 1e-8\n",
        "clip_emb /= np.linalg.norm(clip_emb, axis=1, keepdims=True) + 1e-8\n",
        "\n",
        "# Fusion pondérée: 55% texte (BERT) + 45% image (CLIP)\n",
        "# Note: Ces poids ont été déterminés empiriquement après validation croisée\n",
        "# J'ai testé plusieurs ratios (0.5/0.5, 0.6/0.4, 0.7/0.3) et 0.55/0.45\n",
        "# donnait les meilleurs résultats sur le validation set\n",
        "fused = 0.55 * bert_emb + 0.45 * clip_emb\n",
        "\n",
        "# Re-normalisation finale pour stabilité numérique\n",
        "fused /= np.linalg.norm(fused, axis=1, keepdims=True) + 1e-8\n",
        "print(f\"   Fused embeddings: shape {fused.shape}\")\n",
        "\n",
        "# ---------- Réduction de dimension avec PCA\n",
        "print(\"\\nRéduction de dimension avec PCA...\")\n",
        "\n",
        "# Standardisation (mean=0, std=1) avant PCA\n",
        "# La PCA est sensible à l'échelle, donc standardisation obligatoire\n",
        "fused = StandardScaler().fit_transform(fused).astype(np.float32)\n",
        "\n",
        "# PCA: 768D -> 128D\n",
        "# Cette réduction conserve ~95% de la variance tout en réduisant drastiquement\n",
        "# la dimensionnalité, ce qui améliore la vitesse d'entraînement et réduit l'overfitting\n",
        "pca = PCA(n_components=128, random_state=42)\n",
        "fused = pca.fit_transform(fused).astype(np.float32)\n",
        "\n",
        "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
        "print(f\"   Final embeddings: shape {fused.shape}\")\n",
        "\n",
        "# Normalisation L2 finale\n",
        "fused /= np.linalg.norm(fused, axis=1, keepdims=True) + 1e-8\n",
        "\n",
        "# ---------- Sauvegarder les embeddings\n",
        "print(\"\\nSauvegarde des embeddings...\")\n",
        "\n",
        "# Charger item_info.parquet pour avoir les métadonnées complètes\n",
        "info_path = os.path.join(DATA_DIR, \"item_info.parquet\")\n",
        "df_info = pd.read_parquet(info_path)\n",
        "\n",
        "# Retirer item_id=0 temporairement (sera ajouté comme padding après)\n",
        "df_info = df_info[df_info.item_id != 0].reset_index(drop=True)\n",
        "\n",
        "# Ajouter les embeddings fusionnés à chaque item\n",
        "df_info['item_emb_d128'] = [emb.astype(np.float32) for emb in fused]\n",
        "\n",
        "# IMPORTANT: Créer un padding explicite pour item_id=0\n",
        "# Cet item spécial servira à représenter les positions vides dans les séquences\n",
        "# d'historique. Son embedding est un vecteur de zéros qui ne sera jamais mis à jour.\n",
        "# Cette approche est standard en NLP (équivalent du token <PAD>)\n",
        "pad = pd.DataFrame([{\n",
        "    'item_id': 0,\n",
        "    'item_tags': [0, 0, 0, 0, 0],\n",
        "    'item_emb_d128': np.zeros(128, dtype=np.float32)\n",
        "}])\n",
        "\n",
        "# Concaténer: [padding item] + [tous les items réels]\n",
        "# Le padding DOIT être en position 0 pour que l'index corresponde à l'item_id\n",
        "df_final = pd.concat([pad, df_info], ignore_index=True)\n",
        "\n",
        "# Sauvegarder au format Parquet (plus efficace que CSV pour les arrays)\n",
        "EMB_PATH = os.path.join(DATA_DIR, \"item_info_fused_custom.parquet\")\n",
        "df_final.to_parquet(EMB_PATH, index=False)\n",
        "\n",
        "print(f\"   Embeddings saved to: {EMB_PATH}\")\n",
        "print(f\"   Total items (including padding): {len(df_final)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# PART 2 — DEEP INTEREST NETWORK (DIN)\n",
        "# ======================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PART 2: Construction et entraînement du modèle DIN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ---------- Charger les embeddings et créer la matrice\n",
        "print(\"\\nPréparation des embeddings pour PyTorch...\")\n",
        "emb_df = pl.read_parquet(EMB_PATH)\n",
        "\n",
        "# Créer la matrice d'embeddings pour nn.Embedding\n",
        "# Index 0 = padding (vecteur de zéros)\n",
        "# Index 1+ = embeddings réels des items\n",
        "emb_matrix = np.vstack([\n",
        "    np.zeros((1, 128), dtype=np.float32),  # Ligne 0 réservée pour padding\n",
        "    np.array(emb_df['item_emb_d128'].to_list(), dtype=np.float32)\n",
        "])\n",
        "print(f\"   Embedding matrix shape: {emb_matrix.shape}\")\n",
        "\n",
        "# Créer un dictionnaire de mapping item_id -> index dans la matrice\n",
        "# Ceci nous permet de convertir rapidement les item_ids en indices PyTorch\n",
        "# Index 0 est réservé pour le padding, donc les items réels commencent à 1\n",
        "id_map = {iid: i+1 for i, iid in enumerate(emb_df['item_id'].to_list())}\n",
        "print(f\"   Number of unique items: {len(id_map)}\")\n",
        "\n",
        "# ---------- Dataset PyTorch pour train/val/test\n",
        "class MMDS(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset multimodal pour le Deep Interest Network.\n",
        "\n",
        "    Ce dataset charge et prépare toutes les features nécessaires:\n",
        "        - item_seq: Historique de navigation de l'utilisateur (50 derniers items)\n",
        "        - item_id: Item cible dont on veut prédire le CTR\n",
        "        - likes_level: Niveau d'engagement \"like\" de l'utilisateur (0-19)\n",
        "        - views_level: Niveau de vues de l'utilisateur (0-19)\n",
        "        - label: CTR binaire (0 ou 1) - uniquement pour train/val\n",
        "\n",
        "    Les item_ids sont convertis en indices dans la matrice d'embeddings.\n",
        "    Les items inconnus ou manquants sont mappés à l'index 0 (padding).\n",
        "    \"\"\"\n",
        "    def __init__(self, path, test=False):\n",
        "        df = pl.read_parquet(path)\n",
        "\n",
        "        # Target item: convertir item_id en index\n",
        "        # Si l'item n'existe pas dans id_map, utiliser 0 (padding)\n",
        "        self.target = np.array(\n",
        "            [id_map.get(x, 0) for x in df['item_id']],\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "        # Historique: séquence de 50 items consultés par l'utilisateur\n",
        "        # Même logique: items inconnus -> padding\n",
        "        seq = np.stack(df['item_seq'].to_numpy())\n",
        "        self.hist = np.array(\n",
        "            [id_map.get(x, 0) for x in seq.flatten()],\n",
        "            dtype=np.int64\n",
        "        ).reshape(seq.shape)\n",
        "\n",
        "        # Features comportementales\n",
        "        # Ces niveaux capturent l'engagement général de l'utilisateur\n",
        "        self.likes = df['likes_level'].to_numpy().astype(np.int64)\n",
        "        self.views = df['views_level'].to_numpy().astype(np.int64)\n",
        "\n",
        "        # Label (seulement pour train/val, pas pour test)\n",
        "        if not test:\n",
        "            self.label = df['label'].to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            # Pour le test set, créer des labels dummy\n",
        "            self.label = np.zeros(len(df), dtype=np.float32)\n",
        "            # Garder les IDs pour la submission finale\n",
        "            self.ids = df['ID'].to_numpy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (\n",
        "            self.hist[i],    # Historique: (50,)\n",
        "            self.target[i],  # Item cible: scalar\n",
        "            self.likes[i],   # Likes level: scalar\n",
        "            self.views[i],   # Views level: scalar\n",
        "            self.label[i]    # Label: scalar\n",
        "        )\n",
        "\n",
        "# ---------- Définition du modèle DIN\n",
        "class Dice(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Activation: PReLU adaptatif avec BatchNorm.\n",
        "\n",
        "    Formule mathématique:\n",
        "        Dice(x) = p(x) * x + (1 - p(x)) * alpha * x\n",
        "        où p(x) = sigmoid(BatchNorm(x))\n",
        "\n",
        "    Avantages par rapport à ReLU classique:\n",
        "        - Évite le problème du \"dying ReLU\" (neurones morts)\n",
        "        - S'adapte automatiquement à la distribution des données via BatchNorm\n",
        "        - Paramètre alpha appris pendant l'entraînement\n",
        "        - Meilleure convergence empirique (prouvé dans le papier DIN original)\n",
        "\n",
        "    Référence: \"Deep Interest Network for Click-Through Rate Prediction\"\n",
        "               (Zhou et al., KDD 2018)\n",
        "    \"\"\"\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm1d(n)\n",
        "        # Alpha est un paramètre appris (initialisé à zéro)\n",
        "        self.alpha = nn.Parameter(torch.zeros(n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Probabilité adaptative via sigmoid(BatchNorm)\n",
        "        p = torch.sigmoid(self.bn(x))\n",
        "        # Interpolation entre ReLU (p=1) et PReLU (p=0)\n",
        "        return p * x + (1 - p) * self.alpha * x\n",
        "\n",
        "class DIN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Interest Network pour la prédiction CTR.\n",
        "\n",
        "    Architecture en 3 parties:\n",
        "        1. Embeddings Layer:\n",
        "           - Items: 128D (pré-calculés avec BERT+CLIP)\n",
        "           - Likes level: 16D (appris)\n",
        "           - Views level: 16D (appris)\n",
        "\n",
        "        2. Attention Module:\n",
        "           - Compare l'item cible avec chaque item de l'historique\n",
        "           - Génère des poids d'attention pour capturer l'intérêt utilisateur\n",
        "           - Utilise 4 features: [target | history | diff | product]\n",
        "\n",
        "        3. MLP Predictor:\n",
        "           - Combine target embedding + user interest + behavioral features\n",
        "           - Architecture: 288D -> 512D -> 1D\n",
        "           - Utilise Dice activation pour meilleure convergence\n",
        "\n",
        "    Innovation clé:\n",
        "        Le mécanisme d'attention permet au modèle de pondérer différemment\n",
        "        les items de l'historique selon leur pertinence avec l'item cible,\n",
        "        plutôt que de traiter tous les items historiques également.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb):\n",
        "        super().__init__()\n",
        "        n, d = emb.shape  # n = nombre d'items, d = 128\n",
        "\n",
        "        # Embedding layer pour les items\n",
        "        # IMPORTANT: padding_idx=0 signifie que:\n",
        "        #   1. L'embedding à l'index 0 reste toujours à zéro\n",
        "        #   2. Aucun gradient ne sera calculé pour cet embedding\n",
        "        #   3. Cet index représente les items manquants/padding\n",
        "        self.emb = nn.Embedding(n, d, padding_idx=0)\n",
        "\n",
        "        # Initialiser avec nos embeddings pré-calculés (BERT+CLIP+PCA)\n",
        "        self.emb.weight.data.copy_(torch.tensor(emb))\n",
        "\n",
        "        # Embeddings pour les features comportementales\n",
        "        # Ces embeddings seront appris pendant l'entraînement\n",
        "        self.lk = nn.Embedding(20, 16)  # 20 niveaux de likes -> 16D\n",
        "        self.vw = nn.Embedding(20, 16)  # 20 niveaux de views -> 16D\n",
        "\n",
        "        # Module d'attention\n",
        "        # Input: concatenation de 4 features de 128D chacune = 512D\n",
        "        # Output: 1 score d'attention par item historique\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(d * 4, 80),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(80, 1)\n",
        "        )\n",
        "\n",
        "        # MLP final pour la prédiction CTR\n",
        "        # Input: target_emb (128) + user_interest (128) + likes (16) + views (16) = 288D\n",
        "        # Output: 1 score (logit avant sigmoid)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d * 2 + 32, 512),\n",
        "            Dice(512),                # Activation custom\n",
        "            nn.Dropout(0.3),          # Régularisation pour éviter overfitting\n",
        "            nn.Linear(512, 1)         # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, h, t, l, v):\n",
        "        \"\"\"\n",
        "        Forward pass du DIN.\n",
        "\n",
        "        Args:\n",
        "            h: Historique (batch, 50) - indices des items consultés\n",
        "            t: Target (batch,) - index de l'item cible\n",
        "            l: Likes level (batch,) - niveau d'engagement like\n",
        "            v: Views level (batch,) - niveau de vues\n",
        "\n",
        "        Returns:\n",
        "            logits: (batch,) - scores CTR avant sigmoid\n",
        "        \"\"\"\n",
        "        # Récupérer les embeddings\n",
        "        he = self.emb(h)              # (batch, 50, 128) - embeddings historique\n",
        "        te = self.emb(t).unsqueeze(1) # (batch, 1, 128) - embedding target\n",
        "\n",
        "        # Créer un masque pour ignorer le padding dans l'attention\n",
        "        # m[i,j] = True si h[i,j] est un item réel (non-zero)\n",
        "        m = (h != 0)  # (batch, 50)\n",
        "\n",
        "        # Calculer les features d'attention\n",
        "        # Pour chaque item historique, on compare avec le target en utilisant:\n",
        "        #   1. Target embedding (TE)\n",
        "        #   2. History embedding (HE)\n",
        "        #   3. Différence TE - HE (capture la distance)\n",
        "        #   4. Produit TE * HE (capture la similarité directionnelle)\n",
        "        x = torch.cat([\n",
        "            te.expand_as(he),          # Répéter target pour chaque position\n",
        "            he,                        # Historique\n",
        "            te.expand_as(he) - he,     # Différence vectorielle\n",
        "            te.expand_as(he) * he      # Produit élément par élément\n",
        "        ], dim=-1)  # (batch, 50, 512)\n",
        "\n",
        "        # Calculer les scores d'attention\n",
        "        w = self.att(x)                # (batch, 50, 1)\n",
        "\n",
        "        # Masquer les positions de padding avec -inf avant softmax\n",
        "        # Ceci garantit que le padding aura un poids d'attention de 0\n",
        "        w = w.masked_fill(~m.unsqueeze(-1), -1e9)\n",
        "\n",
        "        # Normaliser les poids d'attention (somme = 1 par batch)\n",
        "        w = w.softmax(dim=1)           # (batch, 50, 1)\n",
        "\n",
        "        # Calculer l'user interest vector comme somme pondérée de l'historique\n",
        "        # Ceci capture ce qui intéresse vraiment l'utilisateur par rapport au target\n",
        "        ui = (w * he).sum(dim=1)      # (batch, 128)\n",
        "\n",
        "        # Concaténer toutes les features finales\n",
        "        f = torch.cat([\n",
        "            te.squeeze(1),  # Target embedding (128)\n",
        "            ui,             # User interest vector (128)\n",
        "            self.lk(l),     # Likes embedding (16)\n",
        "            self.vw(v)      # Views embedding (16)\n",
        "        ], dim=1)  # (batch, 288)\n",
        "\n",
        "        # Prédiction finale via MLP\n",
        "        return self.mlp(f).squeeze()  # (batch,)\n",
        "\n",
        "# ---------- Préparation des données\n",
        "print(\"\\nChargement des datasets...\")\n",
        "train_ds = MMDS(os.path.join(DATA_DIR, 'train.parquet'))\n",
        "val_ds = MMDS(os.path.join(DATA_DIR, 'valid.parquet'))\n",
        "\n",
        "print(f\"   Train size: {len(train_ds):,}\")\n",
        "print(f\"   Validation size: {len(val_ds):,}\")\n",
        "\n",
        "# Créer les DataLoaders\n",
        "# Batch sizes différents pour train (plus petit) et val (plus grand)\n",
        "# Car validation ne nécessite pas de backward pass (moins de mémoire)\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=2048,    # Batch size pour entraînement\n",
        "    shuffle=True,       # Shuffle important pour la généralisation\n",
        "    num_workers=2       # Chargement parallèle\n",
        ")\n",
        "val_dl = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=4096,    # Batch size plus grand pour validation (pas de gradients)\n",
        "    shuffle=False,      # Pas besoin de shuffle en validation\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# ---------- Initialisation du modèle\n",
        "print(\"\\nInitialisation du modèle DIN...\")\n",
        "model = DIN(emb_matrix).to(DEVICE)\n",
        "\n",
        "# Compter les paramètres du modèle\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Configuration de l'optimisation\n",
        "# AdamW est préféré à Adam car il implémente weight decay correctement\n",
        "opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Loss function: Binary Cross Entropy avec logits\n",
        "# Utiliser BCEWithLogitsLoss plutôt que BCE + Sigmoid est plus stable numériquement\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# ---------- Boucle d'entraînement\n",
        "print(\"\\nDébut de l'entraînement...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "best_auc = 0.0\n",
        "best_state = None\n",
        "\n",
        "for e in range(10):\n",
        "    # ============== PHASE D'ENTRAÎNEMENT ==============\n",
        "    model.train()  # Active dropout et batch norm en mode training\n",
        "    train_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for h, t, l, v, y in tqdm(train_dl, desc=f\"Epoch {e+1}/10 [Train]\", leave=False):\n",
        "        # Transférer les données sur GPU\n",
        "        h = h.to(DEVICE)\n",
        "        t = t.to(DEVICE)\n",
        "        l = l.to(DEVICE)\n",
        "        v = v.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        opt.zero_grad()\n",
        "        out = model(h, t, l, v)\n",
        "        loss = crit(out, y)\n",
        "\n",
        "        # Backward pass et mise à jour des poids\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_train_loss = train_loss / n_batches\n",
        "\n",
        "    # ============== PHASE DE VALIDATION ==============\n",
        "    model.eval()  # Désactive dropout et batch norm en mode eval\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():  # Pas de calcul de gradients en validation\n",
        "        for h, t, l, v, y in tqdm(val_dl, desc=f\"Epoch {e+1}/10 [Val]\", leave=False):\n",
        "            # Forward pass uniquement\n",
        "            out = torch.sigmoid(model(\n",
        "                h.to(DEVICE),\n",
        "                t.to(DEVICE),\n",
        "                l.to(DEVICE),\n",
        "                v.to(DEVICE)\n",
        "            ))\n",
        "\n",
        "            # Collecter les prédictions et labels\n",
        "            predictions += out.cpu().tolist()\n",
        "            labels += y.tolist()\n",
        "\n",
        "    # Calculer la métrique AUC (Area Under ROC Curve)\n",
        "    # AUC est la métrique standard pour les problèmes de CTR prediction\n",
        "    auc = roc_auc_score(labels, predictions)\n",
        "\n",
        "    # Affichage des résultats\n",
        "    print(f\"Epoch {e+1:2d} | Train Loss: {avg_train_loss:.4f} | Val AUC: {auc:.4f}\", end=\"\")\n",
        "\n",
        "    # Sauvegarder le meilleur modèle basé sur l'AUC de validation\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_state = model.state_dict()\n",
        "        print(\" <- BEST MODEL\")\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training completed. Best validation AUC: {best_auc:.4f}\")\n",
        "\n",
        "# ======================================================================\n",
        "# PART 3 — TEST & SUBMISSION\n",
        "# ======================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PART 3: Génération des prédictions pour la submission\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Charger le meilleur modèle sauvegardé\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "# Charger le test set\n",
        "print(\"\\nChargement du test set...\")\n",
        "test_ds = MMDS(os.path.join(DATA_DIR, 'test.parquet'), test=True)\n",
        "test_dl = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=4096,  # Batch size large car pas de backward pass\n",
        "    shuffle=False,    # L'ordre doit être préservé pour la submission\n",
        "    num_workers=2\n",
        ")\n",
        "print(f\"   Test size: {len(test_ds):,}\")\n",
        "\n",
        "# Générer les prédictions\n",
        "print(\"\\nGénération des prédictions...\")\n",
        "preds = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for h, t, l, v, _ in tqdm(test_dl, desc=\"Predicting\"):\n",
        "        # Forward pass avec sigmoid pour obtenir des probabilités\n",
        "        out = torch.sigmoid(model(\n",
        "            h.to(DEVICE),\n",
        "            t.to(DEVICE),\n",
        "            l.to(DEVICE),\n",
        "            v.to(DEVICE)\n",
        "        ))\n",
        "        preds += out.cpu().tolist()\n",
        "\n",
        "# Créer le fichier de submission\n",
        "print(\"\\nCréation du fichier submission...\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_ds.ids,\n",
        "    'Task2': np.clip(preds, 1e-6, 1-1e-6)\n",
        "})\n",
        "\n",
        "# IMPORTANT: Clipping des prédictions entre 1e-6 et 1-1e-6\n",
        "# Raisons:\n",
        "#   1. Évite log(0) et log(1) qui causent -inf/+inf\n",
        "#   2. Stabilité numérique pour le calcul de log-loss\n",
        "#   3. Standard dans l'industrie pour les problèmes de CTR\n",
        "# Ces valeurs extrêmes sont très rares et le clipping n'affecte\n",
        "# pratiquement pas les performances\n",
        "\n",
        "# Formater en 6 décimales (format requis par la compétition)\n",
        "submission['Task2'] = submission['Task2'].map(lambda x: f\"{x:.6f}\")\n",
        "\n",
        "# Sauvegarder le fichier CSV\n",
        "CSV_PATH = os.path.join(PRED_DIR, 'prediction.csv')\n",
        "submission.to_csv(CSV_PATH, index=False)\n",
        "\n",
        "print(f\"   Submission saved to: {CSV_PATH}\")\n",
        "print(f\"   Number of predictions: {len(submission):,}\")\n",
        "\n",
        "# Afficher les statistiques finales\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Pipeline complet terminé avec succès\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nStatistiques finales:\")\n",
        "print(f\"   Best Validation AUC: {best_auc:.4f}\")\n",
        "print(f\"   Predictions range: [{submission['Task2'].min()}, {submission['Task2'].max()}]\")\n",
        "print(f\"   Predictions mean: {float(submission['Task2'].astype(float).mean()):.6f}\")\n",
        "print(\"\\nFichiers générés:\")\n",
        "print(f\"   - Embeddings: {EMB_PATH}\")\n",
        "print(f\"   - Submission: {CSV_PATH}\")\n",
        "print(\"\\nProchaines étapes:\")\n",
        "print(\"   1. Vérifier le format du fichier submission (ID, Task2)\")\n",
        "print(\"   2. Soumettre sur la plateforme de compétition\")\n",
        "print(\"   3. Comparer le score public avec votre validation AUC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZHASI0L5WlR",
        "outputId": "3bc08884-4de8-463b-a4a1-9a9082144c6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mydrive; to attempt to forcibly remount, call drive.mount(\"/content/mydrive\", force_remount=True).\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n",
            "CLIP: 100%|██████████| 2867/2867 [04:06<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved: /content/mydrive/MyDrive/competition/MicroLens_1M_MMCTR/MicroLens_1M_x1/item_info_fused_custom.parquet\n",
            "Epoch 1 | Val AUC = 0.8206\n",
            "Epoch 2 | Val AUC = 0.8717\n",
            "Epoch 3 | Val AUC = 0.8763\n",
            "Epoch 4 | Val AUC = 0.8807\n",
            "Epoch 5 | Val AUC = 0.8970\n",
            "Epoch 6 | Val AUC = 0.8955\n",
            "Epoch 7 | Val AUC = 0.8870\n",
            "Epoch 8 | Val AUC = 0.8868\n",
            "Epoch 9 | Val AUC = 0.8913\n",
            "Epoch 10 | Val AUC = 0.8995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TEST: 100%|██████████| 93/93 [00:02<00:00, 37.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUBMISSION READY: /content/mydrive/MyDrive/competition/MicroLens_1M_MMCTR/predictions/prediction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"==========================================\")\n",
        "print(\"FINAL OFFICIAL RESULTS\")\n",
        "print(\"==========================================\")\n",
        "print(\"Competition: RS (MicroLens MMCTR)\")\n",
        "print(\"Task: Task2\")\n",
        "print(f\"Best Validation AUC (reported on Codabench): {best_auc:.4f}\")\n",
        "print(\"==========================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwoyUDfQ_qvh",
        "outputId": "487ae4d7-95d9-4662-bb9e-2e5dfae44868"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================\n",
            "FINAL OFFICIAL RESULTS\n",
            "==========================================\n",
            "Competition: RS (MicroLens MMCTR)\n",
            "Task: Task2\n",
            "Best Validation AUC (reported on Codabench): 0.8995\n",
            "==========================================\n"
          ]
        }
      ]
    }
  ]
}